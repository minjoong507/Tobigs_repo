{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aaa8a01a-a69b-4362-90fa-330f1bb0877f",
    "_uuid": "cdce668d96030d7382a6130bfe6df0780525fabc"
   },
   "source": [
    "<h1>Welcome to my Kernel</h1>\n",
    "\n",
    "### I am learning about some automated tools to Machine Learning and I will try to implement some of them on this  Kernel.\n",
    "<br> <i>*English is not my first language, sorry about any error</i>\n",
    "<h1>Overview</h1>\n",
    "There are 1460 instances of training data and 1460 of test data. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical + Id and SalePrice.<br>\n",
    "<br>\n",
    "Quantitative: <i>1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold</i><br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Qualitative: <i>Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities,</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2e411e00aff546d5f08dfc72b0effcef6190dbf"
   },
   "source": [
    "\n",
    "<h2>I will do some exploration trough  the House Prices, prerpocessing, modeling, set the feature engineering and TPOT model. <h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1b816e5f-aa7e-4d85-8a15-1b625f34f984",
    "_uuid": "5a31295194d9006415d787eb69854f1a2f516eaf"
   },
   "source": [
    "<b>If you like my Kernel, please give me your feedback and votes up =)  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c370423c58d5c9d4b9b2d92c81d1da5b2445a21a"
   },
   "source": [
    "## Importing the librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43899d73-e414-428d-9dac-19ddf0d0ee9d",
    "_uuid": "1a0208221bd29b5e5d86ee10ab2fc49566016502"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8cc6e3de-de72-44c6-8b08-bcccffbb0311",
    "_uuid": "a352f8b4f9189d05797d4ed2e2df3f4ab510d96e"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/datasets/ames/ames_train.csv\")\n",
    "df_test = pd.read_csv(\"../input/datasets/ames/ames_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2bf77af28b740068241e5342438417e64d5d6489"
   },
   "outputs": [],
   "source": [
    "# Concatenating the dataframes in only one\n",
    "df_train['set'] = 'train'\n",
    "df_test['set'] = 'test'\n",
    "df_test[\"SalePrice\"] = np.nan\n",
    "data = pd.concat([df_train, df_test], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04d847ab1259885bf354ecaf3ce2c5961f670869",
    "collapsed": true
   },
   "source": [
    "### Looking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ae093764-e13a-4fca-8d9b-1d2815db457c",
    "_uuid": "229dd150a2e9ccd00e6ac6bb75198d2b24a744bb"
   },
   "outputs": [],
   "source": [
    "#Looking  data\n",
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82e7336d-5a96-40c0-9762-f5a8ff0a1d3f",
    "_uuid": "18f49dc9d2ed93242c611bfc99a4acd594f56a55"
   },
   "source": [
    "I will due with Nan's later, but by now I will fill with \"miss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dab3ee12a2e5d5bc3476a1386142ea582ce65263"
   },
   "outputs": [],
   "source": [
    "for c in ['MiscFeature', 'Alley', 'Fence']:\n",
    "    data[c].fillna('none', inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "76ca3585e7ee8b0047e40750b886c148749873b5"
   },
   "source": [
    "Knowing the type of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "083430fce1e425381b279eadfff9f9d64e4bfdcf"
   },
   "outputs": [],
   "source": [
    "numerical_feats = data.dtypes[data.dtypes != \"object\"].index\n",
    "print(\"Number of Numerical features: \", len(numerical_feats))\n",
    "\n",
    "categorical_feats = data.dtypes[data.dtypes == \"object\"].index\n",
    "print(\"Number of Categorical features: \", len(categorical_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff9e707db06c0e57ba07a72c2e163687157f692a"
   },
   "source": [
    "Nice, now let's explore our features;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e483774b3759b524b00ae7c3363fd61e628423d"
   },
   "source": [
    "## I will start exploring the categorical (object) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "edd76845b3a09b524d492c3856260b9723a30925",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "\n",
    "n = data.select_dtypes(include=object)\n",
    "for column in n.columns:\n",
    "    print(column, ':  ', data[column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0be745d48ef5f86b17bc30a7c50618907d99023a"
   },
   "source": [
    "> Very interesting. Let's plot all this values by our target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "b1a9b40b9d5bdbec2ad5b1ca4f73f2841bc1eb55",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Let's see the distribuition of the categories: \n",
    "for category in list(categorical_feats):\n",
    "    print('#'*35)    \n",
    "    print('Distribuition of feature:', category)\n",
    "    print(data[category].value_counts(normalize=True))\n",
    "    print('#'*35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d114bd65bb57d6ffe81af4f1d3aa8116f566ece9"
   },
   "source": [
    "> Very interesting. We can see that almost all variables have high concentration in 1, 2 or 3 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2652cc3d67ccca2ff0ea085ad1a276c821d03be8"
   },
   "source": [
    "## Now let's plot the categoricals and see the correlation by our target feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20e3b711c32a1508066bc71834dac735a95a2a8e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, nrows=4, \n",
    "                         figsize=(4 * 4, 4 * 4), sharey=True)\n",
    "\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "cols = ['OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual',\n",
    "        'BsmtCond','GarageQual','GarageCond', 'MSSubClass','MSZoning',\n",
    "        'Neighborhood','BldgType','HouseStyle','Heating','Electrical','SaleType']\n",
    "\n",
    "for i, c in zip(np.arange(len(axes)), cols):\n",
    "    ax = sns.boxplot(x=c, y='SalePrice', data=data, ax=axes[i])\n",
    "    ax.set_title(c)\n",
    "    ax.set_xlabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8560c04f6481a72f342767fb7e07770c6a296d88"
   },
   "source": [
    "> Very cool! We can see that some variables have influence on the SalePrice and the OverAllQuality seems the highest influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf8dbf5ceb2beb7de16d403038b56e70741e3ba5"
   },
   "outputs": [],
   "source": [
    "# to categorical feature\n",
    "cols = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\n",
    "        \"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\n",
    "        \"LowQualFinSF\",\"GarageYrBlt\"]\n",
    "\n",
    "for c in cols:\n",
    "    data[c] = data[c].astype(str)\n",
    "\n",
    "# encode quality\n",
    "# Ex(Excellent), Gd（Good）, TA（Typical/Average）, Fa（Fair）, Po（Poor）\n",
    "cols = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond','PoolQC']\n",
    "for c in cols:\n",
    "    data[c].fillna(0, inplace=True)\n",
    "    data[c].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7986700cf7872d635a6d96fdb2b95a49196005f"
   },
   "outputs": [],
   "source": [
    "def pair_features_to_dummies(df, col1, col2, prefix):\n",
    "    d_1 = pd.get_dummies(df[col1].astype(str), prefix=prefix)\n",
    "    d_2 = pd.get_dummies(df[col2].astype(str), prefix=prefix)\n",
    "    for c in list(set(list(d_1.columns) + list(d_2.columns))):\n",
    "        if not c in d_1.columns: d_1[c] = 0\n",
    "        if not c in d_2.columns: d_2[c] = 0\n",
    "    return (d_1 + d_2).clip(0, 1)\n",
    "\n",
    "cond = pair_features_to_dummies(data,'Condition1','Condition2','Condition')\n",
    "exterior = pair_features_to_dummies(data,'Exterior1st','Exterior2nd','Exterior')\n",
    "bsmtftype = pair_features_to_dummies(data,'BsmtFinType1','BsmtFinType2','BsmtFinType') \n",
    "\n",
    "all_data = pd.concat([data, cond, exterior, bsmtftype], axis=1)\n",
    "all_data.drop(['Condition1','Condition2', 'Exterior1st','Exterior2nd','BsmtFinType1','BsmtFinType2'], axis=1, inplace=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "683080b2799591404e23e17ab76e34bbb480aab0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "25a0970778208195c740768d04443e1f934dd578"
   },
   "outputs": [],
   "source": [
    "# fillna\n",
    "for c in ['MiscFeature', 'Alley', 'Fence']:\n",
    "    data[c].fillna('None', inplace=True)\n",
    "    \n",
    "data['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "data.loc[data.GarageYrBlt.isnull(),'GarageYrBlt'] = data.loc[all_data.GarageYrBlt.isnull(),'YearBuilt']\n",
    "\n",
    "data['GarageType'].fillna('None', inplace=True)\n",
    "data['GarageFinish'].fillna(0, inplace=True)\n",
    "\n",
    "for c in ['GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n",
    "    data[c].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b5381fba7831f8b164c1fd6abd0f72c2ba931ad"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "for i, t in data.loc[:, data.columns != 'SalePrice'].dtypes.iteritems():\n",
    "    if t == object:\n",
    "        data[i].fillna(data[i].mode()[0], inplace=True)\n",
    "        data[i] = LabelEncoder().fit_transform(data[i].astype(str))\n",
    "    else:\n",
    "        data[i].fillna(data[i].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "666b545a8a6a1557f4ab2ef9fda41e6a5d9071b3"
   },
   "outputs": [],
   "source": [
    "data['OverallQualCond'] = data['OverallQual'] * data['OverallCond']\n",
    "data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n",
    "data['Interaction'] = data['TotalSF'] * data['OverallQual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d64857b05291be8675572fa14d629b238aa65d16"
   },
   "outputs": [],
   "source": [
    "df_train = data[data['SalePrice'].notnull()]\n",
    "df_test = data[data['SalePrice'].isnull()].drop('SalePrice', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70e239666f517bd0c9d70dc3281ec21383a51b97"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6af5d887ecb5d66278ff77324eb1a7d81370a380"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, nrows=9, figsize=(20, 30))\n",
    "axes = np.ravel(axes)\n",
    "col_name = df_train.corr()['SalePrice'][1:].index\n",
    "for i in range(36):\n",
    "    df_train.plot.scatter(ax=axes[i], x=col_name[i], \n",
    "                          y='SalePrice', c='OverallQual', \n",
    "                          sharey=True, colorbar=False, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4620344799bbb2921fbc9c75d9a0100e94b3874"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9a966725fa7cafddb60c5eef5086ebffb9f321d"
   },
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['TotalSF'] < 6000]\n",
    "df_train = df_train[df_train['TotalBsmtSF'] < 4000]\n",
    "df_train = df_train[df_train['SalePrice'] < 700000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ce389b74352803c8a97dd92ce3ac0a29f1d8829"
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['SalePrice','Id'], axis=1).values\n",
    "y_train = df_train['SalePrice'].values\n",
    "X_test  = df_test.drop(['Id'], axis=1).values\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea310b9953a1665b812dd11af1846055922eb872"
   },
   "source": [
    "## Nice, now, lets import the librarys and build the model pipeline to find the best model to our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "874eca5f0fcbafa8b7ee3ff1f687d41b6fdee7da"
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "######## IMPORTING NECESSARY MODULES AND MODELS ########\n",
    "########################################################\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score # to split the data\n",
    "from sklearn.metrics import explained_variance_score, median_absolute_error, r2_score, mean_squared_error #To evaluate our model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split # Model evaluation\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler # Preprocessing\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, RANSACRegressor, SGDRegressor, HuberRegressor, BayesianRidge # Linear models\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor  # Ensemble methods\n",
    "from xgboost import XGBRegressor, plot_importance # XGBoost\n",
    "from sklearn.svm import SVR, SVC, LinearSVC  # Support Vector Regression\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline # Streaming pipelines\n",
    "from sklearn.decomposition import KernelPCA, PCA # Dimensionality reduction\n",
    "from sklearn.feature_selection import SelectFromModel # Dimensionality reduction\n",
    "from sklearn.model_selection import learning_curve, validation_curve, GridSearchCV # Model evaluation\n",
    "from sklearn.base import clone # Clone estimator\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "396067810f93a6d4816bb880c7c3d72ef379056d"
   },
   "outputs": [],
   "source": [
    "thresh = 5 * 10**(-4)\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "#select features using threshold\n",
    "selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "select_X_train = selection.transform(X_train)\n",
    "# eval model\n",
    "select_X_val = selection.transform(X_test)\n",
    "# test \n",
    "select_X_test = selection.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3619dc871a85dcd64cf5d1b091fa8b88c31e04cf"
   },
   "outputs": [],
   "source": [
    "select_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73f1eecf2df08061a1e3863d4985c4a6cba83849",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "seed = 5\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Ridge\", \n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Ridge\", Ridge(random_state=seed, alpha= 0.1, tol=0.1, solver='auto' ))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Lasso\", \n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Lasso\", Lasso(random_state=seed, tol=0.1))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Elastic\", \n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Lasso\", ElasticNet(random_state=seed, tol=0.1))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_RF_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"RF\", RandomForestRegressor(random_state=seed))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_ET_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"ET\", ExtraTreesRegressor(random_state=seed))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_BR_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"BR\", BaggingRegressor(random_state=seed))]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Hub-Reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Hub-Reg\", HuberRegressor())]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_BayRidge\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"BR\", BayesianRidge())]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_XGB_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"XGBR\", XGBRegressor(seed=seed, n_estimators=300))]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_DT_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"DT_reg\", DecisionTreeRegressor())]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_SVR\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"SVR\",  SVR(kernel='linear', C=1e3, degree=2))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_KNN_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"KNN_reg\", KNeighborsRegressor())]\n",
    "                 )))\n",
    "pipelines.append(\n",
    "                (\"Scaled_ADA-Reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"ADA-reg\", AdaBoostRegressor())\n",
    "                 ]))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Gboost-Reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"GBoost-Reg\", GradientBoostingRegressor())]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_RFR_PCA\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"PCA\", PCA(n_components=3)),\n",
    "                     (\"XGB\", RandomForestRegressor())]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_XGBR_PCA\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"PCA\", PCA(n_components=3)),\n",
    "                     (\"XGB\", XGBRegressor())]\n",
    "                 )))\n",
    "\n",
    "#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\n",
    "scoring = 'r2'\n",
    "n_folds = 7\n",
    "\n",
    "results, names  = [], [] \n",
    "\n",
    "for name, model  in pipelines:\n",
    "    kfold = KFold(n_splits=n_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, select_X_train, y_train, cv= kfold,\n",
    "                                 scoring=scoring, n_jobs=1)    \n",
    "    names.append(name)\n",
    "    results.append(cv_results)    \n",
    "    msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "fig.suptitle('Algorithm Comparison', fontsize=22)\n",
    "ax = fig.add_subplot(111)\n",
    "sns.boxplot(x=names, y=results)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_xlabel(\"Algorithmn Name\", fontsize=20)\n",
    "ax.set_ylabel(\"R Squared Score of Models\", fontsize=18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07fea35189ee1e659dac5a09b565225e909948ad"
   },
   "source": [
    "Wow!! Excellent results. \n",
    "\n",
    "I will implement some of this models to find the best prediction to this competition; \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b41b535ecf25087785e7a47f18f4c4d402b91445"
   },
   "source": [
    "## Testing the \"Featuretools\" library to auto feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "622df6d371df6247b7d34bfe3233d75d90a1ac0e"
   },
   "source": [
    "> importing necessary librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b549aaafc2220bd7e0bea2413df7a19c0946f93"
   },
   "outputs": [],
   "source": [
    "import featuretools as ft #importing the module\n",
    "from featuretools import variable_types as vtypes # importing vtypes to classify or categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f801c4ef8826936d20c89a1ebaa0bbeff677c8a"
   },
   "source": [
    "### Entities and EntitySets\n",
    "The first two concepts of featuretools are entities and entitysets. An entity is simply a table (or a DataFrame if you think in Pandas). An EntitySet is a collection of tables and the relationships between them. Think of an entityset as just another Python data structure, with its own methods and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9a7a2520683565bacc41879eea497a849074496"
   },
   "outputs": [],
   "source": [
    "es = ft.EntitySet(\"house_price\") #Creating new Entityset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2382058e33657307bc9e954c046ebaac93e176b9"
   },
   "source": [
    "### Seting the type of some categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "289f389358e752e5bfcba3b2339c5fcb00a2eddf"
   },
   "outputs": [],
   "source": [
    "#Seting the categorical and ordinal variables\n",
    "house_variable_types = {\n",
    "    'BldgType': vtypes.Categorical, 'BsmtCond': vtypes.Categorical, 'BsmtExposure': vtypes.Categorical, \n",
    "    'BsmtFinType1': vtypes.Categorical, 'BsmtFinType2': vtypes.Categorical, 'BsmtQual': vtypes.Ordinal, \n",
    "    'CentralAir': vtypes.Categorical, 'Id':vtypes.Categorical, 'Exterior2nd': vtypes.Categorical, \n",
    "    'Condition1': vtypes.Categorical, 'Condition2': vtypes.Categorical, 'Electrical': vtypes.Categorical,\n",
    "    'ExterCond': vtypes.Categorical, 'ExterQual': vtypes.Ordinal, 'Exterior1st': vtypes.Categorical, \n",
    "    'Foundation': vtypes.Categorical, 'Functional': vtypes.Categorical, 'GarageCond': vtypes.Categorical, \n",
    "    'GarageFinish': vtypes.Categorical, 'GarageQual': vtypes.Ordinal, 'GarageType': vtypes.Categorical, \n",
    "    'Heating': vtypes.Categorical, 'HeatingQC': vtypes.Categorical, 'HouseStyle': vtypes.Categorical, \n",
    "    'LandContour': vtypes.Categorical, 'LandSlope': vtypes.Categorical, 'LotConfig': vtypes.Categorical, \n",
    "    'LotShape': vtypes.Categorical, 'MSZoning': vtypes.Categorical, 'MasVnrType': vtypes.Categorical, \n",
    "    'Neighborhood': vtypes.Categorical, 'PavedDrive': vtypes.Categorical,'RoofMatl': vtypes.Categorical,\n",
    "    'RoofStyle': vtypes.Categorical, 'SaleCondition': vtypes.Categorical, 'SaleType': vtypes.Categorical, \n",
    "    'Street': vtypes.Categorical, 'MiscFeature': vtypes.Categorical, 'KitchenQual': vtypes.Ordinal, \n",
    "    'Utilities': vtypes.Categorical, 'OverallQual': vtypes.Ordinal, 'PoolQC': vtypes.Categorical, \n",
    "    'Alley': vtypes.Categorical, 'FireplaceQu': vtypes.Categorical\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff61876e25d2d65e7d59b1101987d48578185bf5"
   },
   "source": [
    "### Creating a new entity Id inside the created EntitySet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7ab199f3d2dc68b479c1a07c1f93dbd8c9ea28c"
   },
   "outputs": [],
   "source": [
    "#Creating a new entity from our table (data) with Id and we will put the correct variable types\n",
    "es.entity_from_dataframe(entity_id=\"NewFeatures\",\n",
    "                         dataframe=data, index=\"Id\",\n",
    "                         variable_types=house_variable_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ab488054666952d56bc21e87b7c99645b4d459b"
   },
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6594023e3bba1d714c040764a63f1885b4b5a07a"
   },
   "source": [
    "### Creating a normalized entity to cross throught our main interest table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8e07ae98ceabf98ec131a3710bba1b2c4bef68d"
   },
   "outputs": [],
   "source": [
    "# Creating a new entity using the OverallQuality and the most correlated with our target variables\n",
    "es.normalize_entity('NewFeatures', 'Quality', 'OverallQual',\n",
    "                    additional_variables=['Neighborhood','GarageQual','SaleCondition',\n",
    "                                          'KitchenQual','HouseStyle', 'Condition1'],\n",
    "                    make_time_index=False)\n",
    "### Need I set the PriceSale in any moment? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af0420aaf95b8175c2698aa628b95bbf55dd7c9a"
   },
   "source": [
    "# Someone can clearly explain what are happen when I create the relationships? Also, when I run the DFS... What really happens? \n",
    "\n",
    "How can I set some good practices to better feature engineering? Might can I explicitly set my target? Where I use my target ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "255df88f1a83de96255d33ed04e42941c6b8724c"
   },
   "source": [
    "### Adding some interesting values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18904108260e8d5053b611cb70b282d336b01ee8"
   },
   "outputs": [],
   "source": [
    "# es.add_interesting_values(max_values=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f899182f83e2b2ade0422987f9163f48a755c2f"
   },
   "outputs": [],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=es, \n",
    "                                  target_entity=\"NewFeatures\", \n",
    "                                  max_depth=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0212f82ff69e7b71d7bc85e3e8fce1c614f7c8f0"
   },
   "source": [
    "The diference of depth is that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ee00f46951504e5322f1438493dcd6a1af0b401"
   },
   "source": [
    "### Ok, now let's set our X and y values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c25193619dec9660dccb0701f1154b59a81ba272"
   },
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e3ba473e8f7d1008d49bc9c40b50cf1f7eb43a6"
   },
   "outputs": [],
   "source": [
    "#Let's drop some of outliers \n",
    "feature_matrix = feature_matrix[feature_matrix['TotalSF'] < 6000]\n",
    "feature_matrix = feature_matrix[feature_matrix['TotalBsmtSF'] < 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f7f5d358dc7a2e9a40c2a3b6ce1cd899b5d1ab5"
   },
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cbc74da1b908feafd032f37a252199365280ad93"
   },
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix.reset_index() ## I am reseting to try fix  error\n",
    "feature_matrix = feature_matrix.fillna(-999) ## filling NA's \n",
    "\n",
    "df_train = feature_matrix[feature_matrix['set'] == 1].copy() # spliting the data into df train\n",
    "df_train = df_train[feature_matrix['SalePrice'] < 700000] # EXcluding some outliers \n",
    "\n",
    "df_test = feature_matrix[feature_matrix['set'] == 0].copy() # spliting the data into df test\n",
    "\n",
    "#Deleting some inutil features (SalePrice in df_test was just to better handle with the full dataset)\n",
    "del df_test['SalePrice']\n",
    "del df_train['set']\n",
    "del df_test['set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d4a21ea08d369c5353e2e441aa4502a84cc753b"
   },
   "outputs": [],
   "source": [
    "## Why I got back NaN and/or inifite values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6853fe7312630d54a9ef9321480483a3d033d302"
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['SalePrice','Id'], axis=1).values\n",
    "y_train = df_train['SalePrice'].values\n",
    "X_test  = df_test.drop(['Id'], axis=1).values\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bfe64c4a29ca86ba4129cfa04b59142741e46d92"
   },
   "source": [
    "## Now let's use the selector in the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "0ed264dc54a159ca9f1923f21260312ac469520d"
   },
   "outputs": [],
   "source": [
    "thresh = 5 * 10**(-4)\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "#select features using threshold\n",
    "selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "select_X_train = selection.transform(X_train)\n",
    "# eval model\n",
    "select_X_val = selection.transform(X_test)\n",
    "# test \n",
    "select_X_test = selection.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "feef1602362f7a20b41b67a084f42327563b4bc6"
   },
   "outputs": [],
   "source": [
    "print(select_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "913972eb59ffeb750714b2c1210bf35333e1a410",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "seed = 5\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Ridge\", \n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Ridge\", Ridge(random_state=seed, tol=1 ))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Lasso\", \n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Lasso\", Lasso(random_state=seed, tol=0.1))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Elastic\", \n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Lasso\", ElasticNet(random_state=seed, tol=0.1))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_SVR\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"SVR\",  SVR(kernel='linear', C=1e2, degree=5))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_RF_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"RF\", RandomForestRegressor(random_state=seed))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_ET_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"ET\", ExtraTreesRegressor(random_state=seed))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_BR_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"BR\", BaggingRegressor(random_state=seed))]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Hub-Reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Hub-Reg\", HuberRegressor())]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_BayRidge\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"BR\", BayesianRidge())]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_XGB_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"XGBR\", XGBRegressor(seed=seed))]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_DT_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"DT_reg\", DecisionTreeRegressor())]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_KNN_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"KNN_reg\", KNeighborsRegressor())]\n",
    "                 )))\n",
    "pipelines.append(\n",
    "                (\"Scaled_ADA-Reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"ADA-reg\", AdaBoostRegressor())\n",
    "                 ]))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Gboost-Reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"GBoost-Reg\", GradientBoostingRegressor())]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_RFR_PCA\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"PCA\", PCA(n_components=3)),\n",
    "                     (\"XGB\", RandomForestRegressor())]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_XGBR_PCA\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"PCA\", PCA(n_components=3)),\n",
    "                     (\"XGB\", XGBRegressor())]\n",
    "                 )))\n",
    "\n",
    "#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\n",
    "scoring = 'r2'\n",
    "n_folds = 7\n",
    "\n",
    "results, names  = [], [] \n",
    "\n",
    "for name, model  in pipelines:\n",
    "    kfold = KFold(n_splits=n_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, select_X_train, y_train, cv= kfold,\n",
    "                                 scoring=scoring)    \n",
    "    names.append(name)\n",
    "    results.append(cv_results)    \n",
    "    msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "fig.suptitle('Algorithm Comparison', fontsize=22)\n",
    "ax = fig.add_subplot(111)\n",
    "sns.boxplot(x=names, y=results)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_xlabel(\"Algorithmn Name\", fontsize=20)\n",
    "ax.set_ylabel(\"R Squared Score of Models\", fontsize=18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9044b129bcfc8701b6db493ef7de324268a09ec7"
   },
   "source": [
    "Cool ! I have impelemented my first featuretools solution.\n",
    "\n",
    "We can see a improvement in the \"naked\" models predictions. \n",
    "\n",
    "Now, let's try put it together to autoamted ML library TPOT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da2577862e8de0339ad9196c2b35ed8c1a85a8d9"
   },
   "source": [
    "# IMPLEMENTING TPOT\n",
    "<b>TPOT</b> is a Tree-Based Pipeline Optimization Tool (TPOT) is using genetic programming to find the best performing ML pipelines, and it is built on top of scikit-learn.\n",
    "\n",
    "Once your dataset is cleaned and ready to be used, TPOT will help you with the\n",
    "following steps of your ML pipeline:\n",
    "- Feature preprocessing\n",
    "- Feature construction and selection\n",
    "- Model selection\n",
    "- Hyperparameter optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b2e2e81f485bd47e7a68f19dc0016499c6ceb97"
   },
   "outputs": [],
   "source": [
    "# Importing the necessary library\n",
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8be676f3f2361c5e35e87d27c5575f331e06ddcf"
   },
   "outputs": [],
   "source": [
    "## It's a implementation of some customized models to do in future\n",
    "tpot_config = {\n",
    "    'sklearn.ensemble.GradientBoostingRegressor': {\n",
    "        ''\n",
    "    },\n",
    "    'xgboost.XGBRegressor': {\n",
    "        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n",
    "        'fit_prior': [True, False]\n",
    "    },\n",
    "    'sklearn.naive_bayes.MultinomialNB': {\n",
    "        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n",
    "        'fit_prior': [True, False]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0f4946ce7de389dd8ee15cd5b823647ff903a2d"
   },
   "source": [
    "<b>TPOT </b>is very user-friendly as it's similar to using scikit-learn's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "176acc72562a058c8c3b241d9ab2d7e5a82c1531"
   },
   "outputs": [],
   "source": [
    "# We will create our TPOT regressor with commonly used arguments\n",
    "tpot = TPOTRegressor(verbosity=2, scoring='r2', cv=3, \n",
    "                      n_jobs=-1, generations=6, config_dict='TPOT light',\n",
    "                      population_size=50, random_state=3,\n",
    "                      early_stop = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11168b27bf1886dd36ea3b76d9d4a5c1a2685af1"
   },
   "outputs": [],
   "source": [
    "# Fitting the auto ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f86c8cd4379711fb8f03eefd3ceb022195f78cb2"
   },
   "source": [
    "### When we invoke fit method, TPOT will create generations of populations, seeking best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f3857cc5446741d419ff4190ca302b01bb9ae21"
   },
   "outputs": [],
   "source": [
    "#fiting our tpot auto model\n",
    "tpot.fit(select_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19d11461adeb5172acaaa5fe2493f8e173b4e8b4"
   },
   "source": [
    "Very cool and easy to implement library!!! \n",
    "\n",
    "Now, let's create some predictions to submite on the competition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
